import io
import json
import re
import os
import io
from typing import Optional, List, Tuple
import base64
from PIL import Image

# å¯¼å…¥ComfyUIç›¸å…³æ¨¡å—
try:
    import folder_paths
    from comfy.utils import ProgressBar
except ImportError:
    pass

# å¯¼å…¥Google Gemini API
try:
    import google.generativeai as genai
except ImportError:
    genai = None
    print("[Gemini-Kontext] è­¦å‘Š: æœªæ‰¾åˆ°google-generativeaiåº“ï¼Œè¯·è¿è¡Œ: pip install google-generativeai")

def log(message: str, message_type: str = 'info'):
    """æ—¥å¿—å‡½æ•°"""
    print(f"[Gemini-Kontext] {message}")

def get_api_key(api_name: str) -> str:
    """è·å–APIå¯†é’¥"""
    # æŸ¥æ‰¾api_key.iniæ–‡ä»¶
    current_dir = os.path.dirname(os.path.abspath(__file__))
    api_key_file = None
    
    # åœ¨å¤šä¸ªå¯èƒ½çš„ä½ç½®æŸ¥æ‰¾api_key.ini
    search_paths = [
        os.path.join(current_dir, 'api_key.ini'),
        os.path.join(os.path.dirname(current_dir), 'api_key.ini'),
        os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'ComfyUI_LayerStyle_Advance', 'api_key.ini'),
    ]
    
    for path in search_paths:
        if os.path.exists(path):
            api_key_file = path
            break
    
    if not api_key_file:
        raise FileNotFoundError("æ‰¾ä¸åˆ°api_key.iniæ–‡ä»¶ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®é…ç½®APIå¯†é’¥")
    
    try:
        with open(api_key_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line.startswith(api_name + '='):
                    api_key = line.split('=', 1)[1].strip()
                    if api_key:
                        return api_key
        raise ValueError(f"åœ¨api_key.iniä¸­æ‰¾ä¸åˆ°{api_name}çš„é…ç½®")
    except Exception as e:
        raise Exception(f"è¯»å–APIå¯†é’¥æ—¶å‡ºé”™: {str(e)}")

# Geminiå®‰å…¨è®¾ç½®
# æ›´å®½æ¾çš„å®‰å…¨è®¾ç½®ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
gemini_safety_settings = [
    {
        "category": "HARM_CATEGORY_HARASSMENT",
        "threshold": "BLOCK_ONLY_HIGH"
    },
    {
        "category": "HARM_CATEGORY_HATE_SPEECH", 
        "threshold": "BLOCK_ONLY_HIGH"
    },
    {
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "threshold": "BLOCK_ONLY_HIGH"
    },
    {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "BLOCK_ONLY_HIGH"
    }
]

class LS_GeminiTranslator:
    """
    ä¸“é—¨ç”¨äºç¿»è¯‘çš„GeminièŠ‚ç‚¹ï¼Œæ”¯æŒå¤šç§è¯­è¨€äº’è¯‘
    """
    
    CATEGORY = 'ğŸŒGemini-Kontext'
    FUNCTION = "translate_text"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("translated_text",)
    OUTPUT_IS_LIST = (False,)

    def __init__(self):
        self.NODE_NAME = 'GeminiTranslator'

    @classmethod
    def INPUT_TYPES(cls):
        # Geminiæ¨¡å‹åˆ—è¡¨
        gemini_model_list = [
            "gemini-2.0-flash-exp",
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite", 
            "gemini-2.0-pro",
            "gemini-2.5-flash",
            "gemini-2.5-pro",
            "learnlm-1.5-pro-experimental"
        ]
        
        # æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        language_list = [
            "English",
            "ä¸­æ–‡ (Chinese)",
            "æ—¥æœ¬èª (Japanese)", 
            "í•œêµ­ì–´ (Korean)",
            "FranÃ§ais (French)",
            "Deutsch (German)",
            "EspaÃ±ol (Spanish)",
            "Italiano (Italian)",
            "PortuguÃªs (Portuguese)",
            "Ğ ÑƒÑÑĞºĞ¸Ğ¹ (Russian)",
            "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Arabic)",
            "à¤¹à¤¿à¤¨à¥à¤¦à¥€ (Hindi)",
            "Auto Detect (è‡ªåŠ¨æ£€æµ‹)"
        ]
        
        target_language_list = [
            "English",
            "ä¸­æ–‡ (Chinese)",
            "æ—¥æœ¬èª (Japanese)", 
            "í•œêµ­ì–´ (Korean)",
            "FranÃ§ais (French)",
            "Deutsch (German)",
            "EspaÃ±ol (Spanish)",
            "Italiano (Italian)",
            "PortuguÃªs (Portuguese)",
            "Ğ ÑƒÑÑĞºĞ¸Ğ¹ (Russian)",
            "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Arabic)",
            "à¤¹à¤¿à¤¨à¥à¤¦à¥€ (Hindi)"
        ]
        
        return {
            "required": {
                "model": (gemini_model_list, {"default": "gemini-2.5-flash"}),
                "source_language": (language_list, {"default": "Auto Detect (è‡ªåŠ¨æ£€æµ‹)"}),
                "target_language": (target_language_list, {"default": "English"}),
                "text_to_translate": ("STRING", {
                    "default": "è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬",
                    "multiline": True
                }),
                "temperature": ("FLOAT", {"default": 0.3, "min": 0, "max": 1, "step": 0.1}),
                "max_output_tokens": ("INT", {"default": 2048, "min": 100, "max": 8192, "step": 100}),
                "preserve_formatting": ("BOOLEAN", {"default": True}),
                "add_context": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "context_info": ("STRING", {
                    "default": "ç¿»è¯‘ä¸Šä¸‹æ–‡æˆ–ä¸“ä¸šé¢†åŸŸä¿¡æ¯ï¼ˆå¯é€‰ï¼‰",
                    "multiline": True
                }),
            }
        }

    def translate_text(self, model, source_language, target_language, text_to_translate, 
                      temperature, max_output_tokens, preserve_formatting, add_context, 
                      context_info=""):
        
        try:
            import google.generativeai as genai
        except ImportError:
            error_msg = "è¯·å®‰è£…google-generativeaiåº“: pip install google-generativeai"
            log(error_msg, message_type='error')
            return (error_msg,)
        
        try:
            # é…ç½®API
            api_key = get_api_key('google_api_key')
            genai.configure(api_key=api_key, transport='rest')
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            g_model = genai.GenerativeModel(
                model,
                generation_config={
                    "temperature": temperature,
                    "max_output_tokens": max_output_tokens,
                    "top_p": 0.95,
                    "top_k": 40
                },
                safety_settings=gemini_safety_settings
            )
            
            # æ„å»ºç¿»è¯‘æç¤ºè¯
            source_lang = source_language if source_language != "Auto Detect (è‡ªåŠ¨æ£€æµ‹)" else "automatically detected language"
            
            system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{source_lang}ç¿»è¯‘æˆ{target_language}ã€‚

ç¿»è¯‘è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡çš„æ„æ€å’Œè¯­è°ƒ
2. ä½¿ç”¨è‡ªç„¶ã€æµç•…çš„è¡¨è¾¾
3. å¦‚æœæ˜¯ä¸“ä¸šæœ¯è¯­ï¼Œè¯·ä¿æŒå‡†ç¡®æ€§
4. {'ä¿æŒåŸæ–‡çš„æ ¼å¼å’Œç»“æ„' if preserve_formatting else 'å¯ä»¥è°ƒæ•´æ ¼å¼ä»¥é€‚åº”ç›®æ ‡è¯­è¨€'}
5. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹
"""
            
            if add_context and context_info.strip():
                system_prompt += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context_info}"
            
            # æ„å»ºå®Œæ•´çš„æç¤º
            full_prompt = f"""{system_prompt}

è¦ç¿»è¯‘çš„æ–‡æœ¬ï¼š
{text_to_translate}

ç¿»è¯‘ç»“æœï¼š"""
            
            log(f"{self.NODE_NAME}: æ­£åœ¨ä½¿ç”¨ {model} è¿›è¡Œç¿»è¯‘...")
            log(f"{self.NODE_NAME}: æºè¯­è¨€: {source_language} -> ç›®æ ‡è¯­è¨€: {target_language}")
            
            # å‘é€è¯·æ±‚
            response = g_model.generate_content(full_prompt)
            
            if response and response.text:
                translated_text = response.text.strip()
                log(f"{self.NODE_NAME}: ç¿»è¯‘å®Œæˆ")
                log(f"{self.NODE_NAME}: åŸæ–‡: {text_to_translate[:100]}{'...' if len(text_to_translate) > 100 else ''}")
                log(f"{self.NODE_NAME}: è¯‘æ–‡: {translated_text[:100]}{'...' if len(translated_text) > 100 else ''}")
                return (translated_text,)
            else:
                error_msg = "ç¿»è¯‘å¤±è´¥ï¼šæœªæ”¶åˆ°æœ‰æ•ˆå“åº”"
                log(f"{self.NODE_NAME}: {error_msg}", message_type='error')
                return (error_msg,)
                
        except Exception as e:
            error_msg = f"ç¿»è¯‘å‡ºé”™: {str(e)}"
            log(f"{self.NODE_NAME}: {error_msg}", message_type='error')
            return (error_msg,)

class LS_GeminiBatchTranslator:
    """
    æ‰¹é‡ç¿»è¯‘èŠ‚ç‚¹ï¼Œæ”¯æŒå¤šæ®µæ–‡æœ¬åŒæ—¶ç¿»è¯‘
    """
    
    CATEGORY = 'ğŸŒGemini-Kontext'
    FUNCTION = "batch_translate"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("translated_texts",)
    OUTPUT_IS_LIST = (True,)

    def __init__(self):
        self.NODE_NAME = 'GeminiBatchTranslator'

    @classmethod
    def INPUT_TYPES(cls):
        # åœ¨LS_GeminiBatchTranslatorç±»ä¸­ (ç¬¬255è¡Œ)
        gemini_model_list = [
            "gemini-2.0-flash-exp",
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite",
            "gemini-2.0-pro",
            "gemini-2.5-flash",
            "gemini-2.5-pro",
            "learnlm-1.5-pro-experimental"
        ]
        
        language_list = [
            "English",
            "ä¸­æ–‡ (Chinese)",
            "æ—¥æœ¬èª (Japanese)", 
            "í•œêµ­ì–´ (Korean)",
            "FranÃ§ais (French)",
            "Deutsch (German)",
            "EspaÃ±ol (Spanish)",
            "Auto Detect (è‡ªåŠ¨æ£€æµ‹)"
        ]
        
        return {
            "required": {
                "model": (gemini_model_list, {"default": "gemini-2.5-flash"}),
                "source_language": (language_list, {"default": "Auto Detect (è‡ªåŠ¨æ£€æµ‹)"}),
                "target_language": (language_list[:-1], {"default": "English"}),
                "texts_to_translate": ("STRING", {
                    "default": "æ–‡æœ¬1\n---\næ–‡æœ¬2\n---\næ–‡æœ¬3",
                    "multiline": True
                }),
                "separator": ("STRING", {"default": "---"}),
                "temperature": ("FLOAT", {"default": 0.3, "min": 0, "max": 1, "step": 0.1}),
            }
        }

    def batch_translate(self, model, source_language, target_language, texts_to_translate, 
                       separator, temperature):
        
        try:
            import google.generativeai as genai
        except ImportError:
            error_msg = "è¯·å®‰è£…google-generativeaiåº“: pip install google-generativeai"
            log(error_msg, message_type='error')
            return ([error_msg],)
        
        # åˆ†å‰²æ–‡æœ¬
        text_list = [text.strip() for text in texts_to_translate.split(separator) if text.strip()]
        
        if not text_list:
            return (["æ²¡æœ‰æ‰¾åˆ°è¦ç¿»è¯‘çš„æ–‡æœ¬"],)
        
        try:
            # é…ç½®API
            api_key = get_api_key('google_api_key')
            genai.configure(api_key=api_key, transport='rest')
            
            g_model = genai.GenerativeModel(
                model,
                generation_config={
                    "temperature": temperature,
                    "max_output_tokens": 4096,
                    "top_p": 0.95,
                    "top_k": 40
                },
                safety_settings=gemini_safety_settings
            )
            
            translated_texts = []
            source_lang = source_language if source_language != "Auto Detect (è‡ªåŠ¨æ£€æµ‹)" else "automatically detected language"
            
            log(f"{self.NODE_NAME}: å¼€å§‹æ‰¹é‡ç¿»è¯‘ {len(text_list)} æ®µæ–‡æœ¬")
            
            for i, text in enumerate(text_list):
                try:
                    prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æœ¬ä»{source_lang}ç¿»è¯‘æˆ{target_language}ã€‚
åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šï¼š

{text}
"""
                    
                    response = g_model.generate_content(prompt)
                    
                    if response and response.text:
                        translated_text = response.text.strip()
                        translated_texts.append(translated_text)
                        log(f"{self.NODE_NAME}: å®Œæˆç¬¬ {i+1}/{len(text_list)} æ®µç¿»è¯‘")
                    else:
                        translated_texts.append(f"ç¿»è¯‘å¤±è´¥: ç¬¬{i+1}æ®µ")
                        
                except Exception as e:
                    error_msg = f"ç¬¬{i+1}æ®µç¿»è¯‘å‡ºé”™: {str(e)}"
                    translated_texts.append(error_msg)
                    log(f"{self.NODE_NAME}: {error_msg}", message_type='error')
            
            log(f"{self.NODE_NAME}: æ‰¹é‡ç¿»è¯‘å®Œæˆ")
            return (translated_texts,)
            
        except Exception as e:
            error_msg = f"æ‰¹é‡ç¿»è¯‘å‡ºé”™: {str(e)}"
            log(f"{self.NODE_NAME}: {error_msg}", message_type='error')
            return ([error_msg],)

class LS_GeminiImageAnalyzer:
    """
    Geminiå›¾ç‰‡è¯†åˆ«å’Œåˆ†æèŠ‚ç‚¹
    """
    
    CATEGORY = 'ğŸŒGemini-Kontext'
    FUNCTION = "analyze_image"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("analysis_result",)
    OUTPUT_IS_LIST = (False,)

    def __init__(self):
        self.NODE_NAME = 'GeminiImageAnalyzer'

    @classmethod
    def INPUT_TYPES(cls):
        # æ”¯æŒæœ€æ–°çš„Geminiæ¨¡å‹
        gemini_model_list = [
            "gemini-2.5-flash",
            "gemini-2.5-pro",
            "gemini-2.0-flash-exp",
            "gemini-1.5-flash",
            "gemini-1.5-pro",
            "gemini-1.5-flash-8b"
        ]
        
        # åˆ†æç±»å‹é€‰é¡¹
        analysis_types = [
            "è¯¦ç»†æè¿° (Detailed Description)",
            "ç®€çŸ­æè¿° (Brief Description)", 
            "ç‰©ä½“è¯†åˆ« (Object Detection)",
            "åœºæ™¯åˆ†æ (Scene Analysis)",
            "æ–‡å­—è¯†åˆ« (OCR)",
            "æƒ…æ„Ÿåˆ†æ (Emotion Analysis)",
            "è‰ºæœ¯é£æ ¼åˆ†æ (Art Style Analysis)",
            "è‡ªå®šä¹‰æç¤º (Custom Prompt)"
        ]
        
        # è¾“å‡ºè¯­è¨€é€‰é¡¹
        output_languages = [
            "ä¸­æ–‡ (Chinese)",
            "English",
            "æ—¥æœ¬èª (Japanese)", 
            "í•œêµ­ì–´ (Korean)",
            "FranÃ§ais (French)",
            "Deutsch (German)",
            "EspaÃ±ol (Spanish)"
        ]
        
        return {
            "required": {
                "image": ("IMAGE",),
                "model": (gemini_model_list, {"default": "gemini-2.5-flash"}),
                "analysis_type": (analysis_types, {"default": "è¯¦ç»†æè¿° (Detailed Description)"}),
                "output_language": (output_languages, {"default": "ä¸­æ–‡ (Chinese)"}),
                "temperature": ("FLOAT", {"default": 0.3, "min": 0, "max": 1, "step": 0.1}),
                "max_output_tokens": ("INT", {"default": 2048, "min": 100, "max": 8192, "step": 100}),
            },
            "optional": {
                "custom_prompt": ("STRING", {
                    "default": "è¯·æè¿°è¿™å¼ å›¾ç‰‡",
                    "multiline": True
                }),
                "additional_instructions": ("STRING", {
                    "default": "é¢å¤–çš„åˆ†æè¦æ±‚ï¼ˆå¯é€‰ï¼‰",
                    "multiline": True
                }),
            }
        }

    def tensor_to_pil(self, tensor):
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        import torch
        import numpy as np
        
        # ç¡®ä¿tensoråœ¨CPUä¸Š
        if hasattr(tensor, 'cpu'):
            tensor = tensor.cpu()
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(tensor, torch.Tensor):
            array = tensor.numpy()
        else:
            array = tensor
        
        # å¤„ç†æ‰¹æ¬¡ç»´åº¦
        if len(array.shape) == 4:
            array = array[0]  # å–ç¬¬ä¸€å¼ å›¾ç‰‡
        
        # ç¡®ä¿å€¼åœ¨0-255èŒƒå›´å†…
        if array.max() <= 1.0:
            array = (array * 255).astype(np.uint8)
        else:
            array = array.astype(np.uint8)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        if len(array.shape) == 3:
            if array.shape[2] == 3:  # RGB
                return Image.fromarray(array, 'RGB')
            elif array.shape[2] == 4:  # RGBA
                return Image.fromarray(array, 'RGBA')
        elif len(array.shape) == 2:  # ç°åº¦å›¾
            return Image.fromarray(array, 'L')
        
        raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼ï¼Œshape: {array.shape}")

    def image_to_base64(self, pil_image):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºbase64ç¼–ç """
        # å¦‚æœæ˜¯RGBAï¼Œè½¬æ¢ä¸ºRGB
        if pil_image.mode == 'RGBA':
            # åˆ›å»ºç™½è‰²èƒŒæ™¯
            background = Image.new('RGB', pil_image.size, (255, 255, 255))
            background.paste(pil_image, mask=pil_image.split()[-1])  # ä½¿ç”¨alphaé€šé“ä½œä¸ºmask
            pil_image = background
        elif pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        # å‹ç¼©å›¾åƒä»¥å‡å°‘APIè°ƒç”¨å¤§å°
        max_size = (1024, 1024)
        if pil_image.size[0] > max_size[0] or pil_image.size[1] > max_size[1]:
            pil_image.thumbnail(max_size, Image.Resampling.LANCZOS)
        
        # è½¬æ¢ä¸ºbase64
        buffer = io.BytesIO()
        pil_image.save(buffer, format='JPEG', quality=85)
        img_bytes = buffer.getvalue()
        img_base64 = base64.b64encode(img_bytes).decode('utf-8')
        
        return img_base64

    def get_analysis_prompt(self, analysis_type, output_language, custom_prompt="", additional_instructions=""):
        """æ ¹æ®åˆ†æç±»å‹ç”Ÿæˆæç¤ºè¯"""
        
        # è¯­è¨€æ˜ å°„
        lang_map = {
            "ä¸­æ–‡ (Chinese)": "ä¸­æ–‡",
            "English": "English",
            "æ—¥æœ¬èª (Japanese)": "æ—¥æœ¬èª", 
            "í•œêµ­ì–´ (Korean)": "í•œêµ­ì–´",
            "FranÃ§ais (French)": "FranÃ§ais",
            "Deutsch (German)": "Deutsch",
            "EspaÃ±ol (Spanish)": "EspaÃ±ol"
        }
        
        output_lang = lang_map.get(output_language, "ä¸­æ–‡")
        
        # åŸºç¡€æç¤ºè¯æ¨¡æ¿
        base_prompts = {
            "è¯¦ç»†æè¿° (Detailed Description)": f"è¯·ç”¨{output_lang}è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬åœºæ™¯ã€ç‰©ä½“ã€é¢œè‰²ã€æ„å›¾ã€æ°›å›´ç­‰å„ä¸ªæ–¹é¢ã€‚",
            "ç®€çŸ­æè¿° (Brief Description)": f"è¯·ç”¨{output_lang}ç®€æ´åœ°æè¿°è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å†…å®¹ã€‚",
            "ç‰©ä½“è¯†åˆ« (Object Detection)": f"è¯·ç”¨{output_lang}è¯†åˆ«å¹¶åˆ—å‡ºå›¾ç‰‡ä¸­çš„æ‰€æœ‰ç‰©ä½“å’Œå…ƒç´ ã€‚",
            "åœºæ™¯åˆ†æ (Scene Analysis)": f"è¯·ç”¨{output_lang}åˆ†æå›¾ç‰‡çš„åœºæ™¯ç±»å‹ã€ç¯å¢ƒç‰¹å¾å’Œæ•´ä½“æ°›å›´ã€‚",
            "æ–‡å­—è¯†åˆ« (OCR)": f"è¯·ç”¨{output_lang}è¯†åˆ«å¹¶æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ã€‚",
            "æƒ…æ„Ÿåˆ†æ (Emotion Analysis)": f"è¯·ç”¨{output_lang}åˆ†æå›¾ç‰‡ä¼ è¾¾çš„æƒ…æ„Ÿã€æ°›å›´å’Œæ„Ÿå—ã€‚",
            "è‰ºæœ¯é£æ ¼åˆ†æ (Art Style Analysis)": f"è¯·ç”¨{output_lang}åˆ†æå›¾ç‰‡çš„è‰ºæœ¯é£æ ¼ã€æŠ€æ³•å’Œç¾å­¦ç‰¹å¾ã€‚",
            "è‡ªå®šä¹‰æç¤º (Custom Prompt)": custom_prompt if custom_prompt.strip() else f"è¯·ç”¨{output_lang}æè¿°è¿™å¼ å›¾ç‰‡ã€‚"
        }
        
        prompt = base_prompts.get(analysis_type, base_prompts["è¯¦ç»†æè¿° (Detailed Description)"])
        
        if additional_instructions.strip():
            prompt += f"\n\né¢å¤–è¦æ±‚ï¼š{additional_instructions}"
        
        return prompt

    def analyze_image(self, image, model, analysis_type, output_language, temperature, 
                     max_output_tokens, custom_prompt="", additional_instructions=""):
        
        try:
            import google.generativeai as genai
        except ImportError:
            error_msg = "è¯·å®‰è£…google-generativeaiåº“: pip install google-generativeai"
            log(error_msg, message_type='error')
            return (error_msg,)
        
        try:
            # é…ç½®API
            api__key = get_api_key('google_api_key')
            genai.configure(api_key=api_key, transport='rest')
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            g_model = genai.GenerativeModel(
                model,
                generation_config={
                    "temperature": temperature,
                    "max_output_tokens": max_output_tokens,
                    "top_p": 0.95,
                    "top_k": 40
                },
                safety_settings=gemini_safety_settings
            )
            
            # è½¬æ¢å›¾åƒ
            pil_image = self.tensor_to_pil(image)
            
            # ç”Ÿæˆåˆ†ææç¤ºè¯
            prompt = self.get_analysis_prompt(analysis_type, output_language, custom_prompt, additional_instructions)
            
            log(f"{self.NODE_NAME}: æ­£åœ¨ä½¿ç”¨ {model} åˆ†æå›¾ç‰‡...")
            log(f"{self.NODE_NAME}: åˆ†æç±»å‹: {analysis_type}")
            log(f"{self.NODE_NAME}: è¾“å‡ºè¯­è¨€: {output_language}")
            
            # å‘é€è¯·æ±‚
            response = g_model.generate_content([prompt, pil_image])
            
            if response and response.text:
                analysis_result = response.text.strip()
                log(f"{self.NODE_NAME}: å›¾ç‰‡åˆ†æå®Œæˆ")
                log(f"{self.NODE_NAME}: åˆ†æç»“æœ: {analysis_result[:100]}{'...' if len(analysis_result) > 100 else ''}")
                return (analysis_result,)
            else:
                error_msg = "å›¾ç‰‡åˆ†æå¤±è´¥ï¼šæœªæ”¶åˆ°æœ‰æ•ˆå“åº”"
                log(f"{self.NODE_NAME}: {error_msg}", message_type='error')
                return (error_msg,)
                
        except Exception as e:
            error_msg = f"å›¾ç‰‡åˆ†æå‡ºé”™: {str(e)}"
            log(f"{self.NODE_NAME}: {error_msg}", message_type='error')
            return (error_msg,)

class LS_GeminiKontextOptimizer:
    """
    æ”¯æŒå¤šè¯­è¨€è¾“å…¥ï¼Œè¾“å‡ºç¬¦åˆKontextæ ¼å¼çš„è‹±æ–‡æç¤ºè¯
    """
    
    CATEGORY = 'ğŸŒGemini-Kontext'  # ä¿®æ”¹è¿™ä¸€è¡Œ
    FUNCTION = "optimize_kontext_prompt"
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("optimized_prompt", "analysis_details")
    OUTPUT_IS_LIST = (False, False)

    def __init__(self):
        self.NODE_NAME = 'GeminiKontextOptimizer'

    @classmethod
    def INPUT_TYPES(cls):
        # Geminiæ¨¡å‹åˆ—è¡¨
        gemini_model_list = [
            "gemini-2.0-flash-exp",
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite",
            "gemini-2.0-pro",
            "gemini-2.5-flash",
            "gemini-2.5-pro",
            "learnlm-1.5-pro-experimental"
        ]
        
        # ä¼˜åŒ–çº§åˆ«é€‰é¡¹
        optimization_levels = [
            "Basic (åŸºç¡€ä¼˜åŒ–)",
            "Advanced (é«˜çº§ä¼˜åŒ–)", 
            "Professional (ä¸“ä¸šä¼˜åŒ–)"
        ]
        
        # ç¼–è¾‘ç±»å‹
        edit_types = [
            "Object Modification (ç‰©ä½“ä¿®æ”¹)",
            "Background Change (èƒŒæ™¯æ›´æ¢)",
            "Style Transfer (é£æ ¼è½¬æ¢)",
            "Character Consistency (è§’è‰²ä¸€è‡´æ€§)",
            "Text Editing (æ–‡æœ¬ç¼–è¾‘)",
            "Complex Transformation (å¤æ‚å˜æ¢)",
            "Auto Detect (è‡ªåŠ¨æ£€æµ‹)"
        ]
        
        # æ‰§è¡Œåæ“ä½œé€‰é¡¹
        post_execution_options = [
            "None (æ— æ“ä½œ)",
            "Fixed Seed (å›ºå®šç§å­)",
            "Random Seed (éšæœºç§å­)"
        ]
        
        return {
            "required": {
                "image": ("IMAGE",),
                "user_description": ("STRING", {
                    "default": "æˆ‘å¸Œæœ›è¯¥äººç‰©ä¸åšä»»ä½•å˜åŒ–ï¼ŒèƒŒæ™¯å˜ä¸ºæ²™æ»©",
                    "multiline": True
                }),
                "model": (gemini_model_list, {"default": "gemini-2.5-flash"}),
                "edit_type": (edit_types, {"default": "Auto Detect (è‡ªåŠ¨æ£€æµ‹)"}),
                "optimization_level": (optimization_levels, {"default": "Advanced (é«˜çº§ä¼˜åŒ–)"}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0, "max": 1, "step": 0.1}),
                "max_output_tokens": ("INT", {"default": 1024, "min": 100, "max": 4096, "step": 100}),
                "post_execution": (post_execution_options, {"default": "None (æ— æ“ä½œ)"}),
                "stop_on_failure": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "additional_context": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "å¯é€‰ï¼šæä¾›é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"
                }),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
            }
        }

    def get_kontext_system_prompt(self, optimization_level: str, edit_type: str) -> str:
        """
        ç”ŸæˆKontextä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯ï¼Œè®©Geminiè‡ªå·±è¯»å–å®˜æ–¹æ–‡æ¡£
        """
        base_prompt = """
You are an expert Kontext image editing prompt optimizer. 

IMPORTANT: First, please read and understand the official Kontext documentation at this link:
https://docs.bfl.ai/guides/prompting_guide_kontext_i2i#prompt-precision%3A-from-basic-to-comprehensive

After reading the official documentation, follow these requirements:

1. **Analyze the provided image** to understand:
   - Main subjects, characters, objects
   - Background and environment
   - Style, lighting, colors
   - Any text elements

2. **Transform user descriptions** into optimal Kontext prompts based on the official guidelines you just read

3. **Output requirements**:
   - Generate ONLY the optimized prompt in English
   - NO explanations, NO additional text
   - Follow the exact patterns from the official documentation
   - Be specific and detailed but concise
   - Focus on the exact change requested

4. **Apply official best practices** from the documentation for:
   - Object modifications
   - Style transfers
   - Character consistency
   - Background changes
   - Text editing
   - Complex transformations

Remember: Read the official documentation first, then apply those guidelines to optimize the user's request.
"""
        
        return base_prompt

    def optimize_kontext_prompt(self, image, user_description, model, edit_type, optimization_level, temperature, max_output_tokens, post_execution, stop_on_failure, additional_context="", seed=-1):
        """
        ä¼˜åŒ–Kontextæç¤ºè¯çš„ä¸»è¦æ–¹æ³•
        """
        import random
        
        try:
            # å¤„ç†ç§å­è®¾ç½®
            if post_execution == "Random Seed (éšæœºç§å­)":
                current_seed = random.randint(0, 0xffffffffffffffff)
            elif post_execution == "Fixed Seed (å›ºå®šç§å­)" and seed != -1:
                current_seed = seed
            else:
                current_seed = -1  # é»˜è®¤å€¼
            
            # è·å–APIå¯†é’¥å¹¶é…ç½®Gemini
            api_key = get_api_key('google_api_key')
            genai.configure(api_key=api_key)
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            generation_config = {
                "temperature": temperature,
                "top_p": 0.95,
                "top_k": 40,
                "max_output_tokens": max_output_tokens,
            }
            
            model_instance = genai.GenerativeModel(
                model_name=model,
                generation_config=generation_config,
                safety_settings=gemini_safety_settings
            )
            
            # è½¬æ¢å›¾åƒ
            pil_image = self.tensor_to_pil(image)
            image_base64 = self.image_to_base64(pil_image)
            
            # åˆ†æå›¾åƒ
            log("æ­£åœ¨åˆ†æå›¾åƒå†…å®¹...")
            image_analysis = self.analyze_image_for_kontext(image_base64, model_instance)
            
            # æ„å»ºä¼˜åŒ–æç¤ºè¯
            system_prompt = self.get_kontext_system_prompt(optimization_level, edit_type)
            
            optimization_prompt = f"""
{system_prompt}

IMAGE ANALYSIS:
{image_analysis}

USER REQUEST: {user_description}

ADDITIONAL CONTEXT: {additional_context if additional_context else "None"}

TASK: 
1. First read the official Kontext documentation at: https://docs.bfl.ai/guides/prompting_guide_kontext_i2i#prompt-precision%3A-from-basic-to-comprehensive
2. Based on the documentation and image analysis, generate the optimal Kontext prompt
3. Output ONLY the prompt, no explanations

OUTPUT:
"""
            
            # å‘é€è¯·æ±‚
            log("æ­£åœ¨è®©Geminiè¯»å–å®˜æ–¹æ–‡æ¡£å¹¶ä¼˜åŒ–æç¤ºè¯...")
            response = model_instance.generate_content([
                optimization_prompt,
                {
                    "mime_type": "image/jpeg",
                    "data": image_base64
                }
            ])
            
            # å¤„ç†ç»“æœ
            try:
                result_text = response.text.strip()
                
                # æ¸…ç†è¾“å‡ºï¼Œåªä¿ç•™æç¤ºè¯
                if "OUTPUT:" in result_text:
                    result_text = result_text.split("OUTPUT:")[-1].strip()
                
                # ç§»é™¤å¤šä½™çš„æ ¼å¼åŒ–
                lines = result_text.split('\n')
                clean_lines = []
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('Optimized Prompt:') and not line.startswith('**'):
                        clean_lines.append(line)
                
                result_text = ' '.join(clean_lines).strip()
                
                log("Kontextæç¤ºè¯ä¼˜åŒ–å®Œæˆ")
                return (result_text, f"ä¼˜åŒ–å®Œæˆ - æ¨¡å‹: {model} (å·²è¯»å–å®˜æ–¹æ–‡æ¡£)")
                
            except Exception as e:
                error_msg = f"Kontextæç¤ºè¯ä¼˜åŒ–å¤±è´¥: {str(e)}"
                log(error_msg, "error")
                if stop_on_failure:
                    return (f"é”™è¯¯: {str(e)}", error_msg)
                else:
                    return (f"ä¼˜åŒ–å¤±è´¥: {str(e)}", error_msg)
        
        except Exception as e:
            error_msg = f"Kontextæç¤ºè¯ä¼˜åŒ–å‡ºé”™: {str(e)}"
            log(error_msg, "error")
            return (f"é”™è¯¯: {str(e)}", error_msg)

    def tensor_to_pil(self, tensor):
        """å°†tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        import torch
        import numpy as np
        
        # ç¡®ä¿tensoråœ¨CPUä¸Š
        if hasattr(tensor, 'cpu'):
            tensor = tensor.cpu()
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if len(tensor.shape) == 4:
            # æ‰¹æ¬¡ç»´åº¦ï¼Œå–ç¬¬ä¸€ä¸ª
            tensor = tensor[0]
        
        # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
        if tensor.max() <= 1.0:
            tensor = tensor * 255
        
        # è½¬æ¢ä¸ºuint8
        tensor = tensor.clamp(0, 255).byte()
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if hasattr(tensor, 'numpy'):
            array = tensor.numpy()
        else:
            array = np.array(tensor)
        
        # ç¡®ä¿æ˜¯HWCæ ¼å¼
        if len(array.shape) == 3 and array.shape[0] in [1, 3, 4]:
            array = np.transpose(array, (1, 2, 0))
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        if array.shape[2] == 1:
            array = array.squeeze(2)
            pil_image = Image.fromarray(array, mode='L')
        elif array.shape[2] == 3:
            pil_image = Image.fromarray(array, mode='RGB')
        elif array.shape[2] == 4:
            pil_image = Image.fromarray(array, mode='RGBA')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒé€šé“æ•°: {array.shape[2]}")
        
        return pil_image
    
    def image_to_base64(self, pil_image):
        """å°†PILå›¾åƒè½¬æ¢ä¸ºbase64ç¼–ç """
        import io
        
        # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå¦‚æœä¸æ˜¯çš„è¯ï¼‰
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')
        
        # ä¿å­˜åˆ°å­—èŠ‚æµ
        buffer = io.BytesIO()
        pil_image.save(buffer, format='JPEG', quality=95)
        
        # è½¬æ¢ä¸ºbase64
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
        
        return image_base64

    def analyze_image_for_kontext(self, image_base64, model_instance):
        """ä¸ºKontextä¼˜åŒ–ä¸“é—¨åˆ†æå›¾åƒå†…å®¹"""
        try:
            analysis_prompt = """
è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œé‡ç‚¹å…³æ³¨ä»¥ä¸‹æ–¹é¢ï¼š
1. ä¸»è¦å¯¹è±¡å’Œäººç‰©
2. èƒŒæ™¯ç¯å¢ƒå’Œåœºæ™¯
3. è‰ºæœ¯é£æ ¼å’Œè§†è§‰ç‰¹å¾
4. é¢œè‰²ã€å…‰ç…§å’Œæ„å›¾
5. ä»»ä½•æ–‡å­—å…ƒç´ 

è¯·ç”¨ç®€æ´çš„è‹±æ–‡æè¿°ï¼Œä¸ºåç»­çš„Kontextç¼–è¾‘æä¾›å‡†ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
"""
            
            response = model_instance.generate_content([
                analysis_prompt,
                {
                    "mime_type": "image/jpeg",
                    "data": image_base64
                }
            ])
            
            if response and response.text:
                return response.text.strip()
            else:
                return "Unable to analyze image content"
                
        except Exception as e:
            log(f"å›¾åƒåˆ†æå¤±è´¥: {str(e)}", "error")
            return "Image analysis failed"

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "GeminiTranslator": LS_GeminiTranslator,
    "GeminiBatchTranslator": LS_GeminiBatchTranslator,
    "GeminiImageAnalyzer": LS_GeminiImageAnalyzer,
    "GeminiKontextOptimizer": LS_GeminiKontextOptimizer,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GeminiTranslator": "ğŸŒ Gemini Translator",
    "GeminiBatchTranslator": "ğŸŒ Gemini Batch Translator", 
    "GeminiImageAnalyzer": "ğŸ–¼ï¸ Gemini Image Analyzer",
    "GeminiKontextOptimizer": "âœ¨ Gemini Kontext Optimizer",
}
